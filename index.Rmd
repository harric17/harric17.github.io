---
=======
title       : Class Project
subtitle    : Developing Data Products, June 2014
author      : harric17
>>>>>>> FETCH_HEAD
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

<<<<<<< HEAD
## Read-And-Delete

1. Edit YAML front matter
2. Write using R Markdown
3. Use an empty line followed by three dashes to separate slides!

--- .class #id 

## Slide 2

##Test 123
=======
## Introduction

A significant challenge to today's data scientists is extracting meaningful insights from the large volumes of data that are continuously being collected from a variety of sources.   The goal of this analysis was to develop an accurate algorithm for predicting the manner of exercise being performed by 6 participants based on measurements collected from inexpensive personal activity sensors.


--- .class #id 

## Methods

<p>The training dataset contained 19622 observations across 160 variables.  The testing set contained 20 observations, though it did not contain information related to the outcome measure.  The outcome variable of interest, classe, is a categorical variable consisting of 5 different levels.   A random forest model was fit to predict this categorical outcome.</p>
<p>
There was substantial data management required for this analysis.  First, five variables were excluded as obvious noise: X, problem_id, raw_timestamp_part_1, raw_timestamp_part_2, and cvtd_timestamp.  Next, variables that had exclusively missing values in the test set were also excluded.  This resulted in a dataset with 55 predictor variables, two of which were factors with the remaining numeric. </p>

<p>The training dataset was then ultimately divided.  Using a random uniform number generator a small subset of exactly 3000 observations was created and used to train the random forest model.  The remaining data was set aside for further validation of the model.  Ultimately the smaller training subset contained levels of the factor variables that were not in the test data set, so the two factor variables were also excluded from the analysis.  A forest of 500 trees was created, and the number of variables randomly chosen at each split was left as the default value of 7.</p>

```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
``` 

---
## Results
The out of bag error rate for the resulting random forest model was 2.63%, or 97.37% accuracy.  Class errors for the out of bag data are displayed below in Table 1.

Table 1: Confusion matrix from OOB data
```{r, echo=FALSE}
modFit$confusion
```
The random forest model performed similarly well on the validation dataset, predicting the outcome with 97.38% accuracy.  The top 10 variables in terms of importance for this forest are presented in Table 2 on the next slide.  On the test data set, 19 of 20 observations were correctly predicted.

---
## Results

Table 2: Top 10 variables by importance
```{r, echo=FALSE}
data.frame(importance)
```

---
## Conclusions

<p>Many potential predictors were excluded from this analysis and moreover the final model was trained on only about 15% of the available data.  Nonetheless a highly accurate model was constructed using random forests.  The out of sample error from both the out of bag data and validation set are both less than 3%.
</p>
<p>Going forward several more investigative analyses could be performed.  For example this data could be further explored using a complete case analysis, which would yeild information about the predictive importance of the variables that were excluded in this analysis due to a large number of missing values.  Another additional future analysis might be creating a random forest using only the most important variables to see if model accuracy was similarly high.  If so this would minimize the amount of data needed to be collected.  A third future analysis might involve ways of better trying to distinguish the levels of the outcome that the model had problems with, e.g. class D from class C (see Table 1).
</p>
>>>>>>> FETCH_HEAD


