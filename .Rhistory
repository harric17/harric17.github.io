varImpPlot(fit)
plot(fit$err.rate[,1],type="l",col="blue", xlab="Number of Trees",ylab="OOB Error Rate")
set.seed(424)
fit = randomForest(dcctx~.,data=na.roughfix(data), ntree=1000)
plot(fit$err.rate[,1],type="l",col="blue", xlab="Number of Trees",ylab="OOB Error Rate")
varImpPlot(fit)
fit = randomForest(dcctx~.,data=na.roughfix(data), ntree=5000)
plot(fit$err.rate[,1],type="l",col="blue", xlab="Number of Trees",ylab="OOB Error Rate")
varImpPlot(fit)
plot(fit$err.rate[,1],type="l",col="blue", xlab="Number of Trees",ylab="OOB Error Rate")
varImpPlot(fit)
plot(fit$err.rate[,1],type="l",col="blue", xlab="Number of Trees",ylab="OOB Error Rate")
plot(fit$err.rate[,1],type="l",col="blue", xlab="Number of Trees",ylab="OOB Error Rate")
varImpPlot(fit)
names(fit)
fit$importance
sort(fit$importance)
sort(fit$importance,descending)
sort(fit$importance,decreasing=T)
varImpPlot(fit,n.var=10)
varImpPlot(fit,n.var=20)
varImpPlot(fit,n.var=25)
varImpPlot(fit,n.var=25,cex=1.5)
varImpPlot(fit,n.var=25,cex=1.5)
varImpPlot(fit,n.var=23,cex=1.5)
Performance <-
matrix(c(794, 86, 150, 570),
nrow = 2,
dimnames = list("1st Survey" = c("Approve", "Disapprove"),
"2nd Survey" = c("Approve", "Disapprove")))
Performance
mytable <-  matrix(c(9, 22, 1810, 6400), nrow = 2)
mytable
mytable <-  as.table(matrix(c(9, 22, 1810, 6400), nrow = 2))
mytable
prop.test(mytable)
mytable <-  as.table(matrix(c(9, 22, 1810, 6400), nrow = 2))
rownames(mytable)[1] = "H"
rownames(mytable)[1] = "AB"
mytable
mytable <-  as.table(matrix(c(9, 22, 1810, 6400), nrow = 2))
rownames(mytable)[1] = "H"
rownames(mytable)[2] = "AB"
mytable
mytable <-  as.table(matrix(c(9, 22, 1810, 6400), nrow = 2))
rownames(mytable)[1] = "H"
rownames(mytable)[2] = "AB"
colnames(mytable)[1] = "vs. Buerhle"
colnames(mytable)[2] = "vs. others"
mytable
prop.test(mytable)
library(caret)
library(caret)
library(kernlab)
library(kernlab)
install.packages("kernlab")
library(kernlab)
library(kernlab)
data(spam)
inTrain = createDataPartition(y=spam$type,p=.75,list=FALSE)
training = spam[inTrain,]
testing = spam[-inTrain,]
dim(training)
dim(testing)
set.seed(32343)
modelFit = train(type ~.,data=training,method="glm")
set.seed(32343)
modelFit = train(type ~ ., data=training, method="glm")
install.packages("e1071")
library(e1071)
modelFit = train(type ~ ., data=training, method="glm")
warnings()
modelFit
library(caret)
modelFit$finalModel
predictions = predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
set.seed(32323)
folds = createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)
library(caret)
library(e1071)
library(kernlab)
data(spam)
inTrain = createDataPartition(y=spam$type,p=.75,list=FALSE)
training = spam[inTrain,]
testing = spam[-inTrain,]
dim(training)
set.seed(32323)
folds = createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)
sapply(folds,length)
folds = createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)
set.seed(32323)
folds = createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)
sapply(folds,length)
folds = createTimeSlices(y=time,initialWindow=20,horizon=10)
set.seed(32323)
tme = 1:1000
folds = createTimeSlices(y=tme,initialWindow=20,horizon=10)
folds
modelFit = train(type~.,data=training,method="glm")
modelFit = train(type~.,data=training,method="glm")
install.packages("ISLR")
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
inTrain = createDataPartition(y=Wage$wage,p=.7,list=FALSE)
training = Wage[inTrain,]
testing = Wage[-inTrain,]
dim(training)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,data=training,color=jobclass)
qplot(age,wage,data=training,color=jobclass)+geom_smooth(method="lm",formula=y~x)
qplot(age,wage,data=training,color=education)+geom_smooth(method="lm",formula=y~x)
library(Hmisc)
cutWage = cut2(training$wage,g=3)
table(cutWage)
qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
qplot(wage,color=education,data=training,geom="density")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[testIndex,]
testing = adData[-testIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
inTrain
training = mixtures[trainIndex,]
testing = mixtures[-trainIndex,]
mixtures
names(mixtures)
qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
cutWage = cut2(training$wage,g=3)#makes cuts to the data based on quantiles
qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
cutWage = cut2(training$wage,g=3)#makes cuts to the data based on quantiles
cutWage = cut2(training$wage,g=3)
=
cutWage = cut2(training$wage,g=3)
inTrain = createDataPartition(y=Wage$wage,p=.7,list=FALSE)
training = Wage[inTrain,]
testing = Wage[-inTrain,]
dim(training)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training,color=jobclass)
qplot(age,wage,data=training,color=education)+geom_smooth(method="lm",formula=y~x)
cutWage = cut2(training$wage,g=3)
#makes cuts to the data based on quantiles
table(cutWage)
qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
qplot(age,wage,data=training,color=jobclass)
qplot(CompressiveStrength,FlyAsh,data=training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[trainIndex,]
testing = mixtures[-trainIndex,]
qplot(CompressiveStrength,FlyAsh,data=training)
mixtures
names(mixtures)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[trainIndex,]
testing = mixtures[-trainIndex,]
names(mixtures)
qplot(CompressiveStrength,FlyAsh,data=training)
names(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain,]
testing = mixtures[-inTrain,]
names(mixtures)
qplot(CompressiveStrength,FlyAsh,data=training)
names(training)
levels(mixtures$FlyAsh)
mixtures$FlyAsh
cutFly = cut2(mictures$FlyAsh,g=2)
cutFly = cut2(mixtures$FlyAsh,g=2)
qplot(CompressiveStrength,FlyAsh,data=training,color=cutFly)
cutFly
cutFly = cut2(mixtures$FlyAsh,g=3)
table(mixtures$FlyAsh)
)
qplot(CompressiveStrength,FlyAsh,data=training)
qplot(y=CompressiveStrength,x=FlyAsh,data=training)
names(mixtures)
mixtures$ind = 1:length(mixtures)
mixtures$myind = 1:length(mixtures)
length(mixtures)
mixtures$myind = 1:dim(mixtures)[2]
mixtures$myind = 1:dim(mixtures)[1]
qplot(y=CompressiveStrength,x=myind,data=training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
mixtures$myind = 1:dim(mixtures)[1]
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain,]
testing = mixtures[-inTrain,]
names(mixtures)
qplot(y=CompressiveStrength,x=myind,data=training)
cutFly = cut2(training$FlyAsh,g=3)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
cutFly = cut2(training$FlyAsh,g=4)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
cutFly = cut2(training$FlyAsh,g=5)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
cutFly = cut2(training$FlyAsh,g=10)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
cutFly = cut2(training$FlyAsh,g=100)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
cutFly = cut2(training$FlyAsh,g=3)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
cutFly = cut2(training$FlyAsh,g=2)
qplot(y=CompressiveStrength,x=myind,data=training,color=cutFly)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training)
hist(training$SuperPlasticizer)
training$SuperPlasticizer
names(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$SuperPlasticizer
head(training$Superplasticizer)
levels(training$Superplasticizer)
table(training$Superplasticizer)
min(training$Superplasticizer)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
names(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
modFitRF = train(y~.,method="rf",data=vowel.train)
set.seed(33833)
modFitB = train(y~.,method="gbm",data=vowel.train,verbose=FALSE)
confusionMatrix(predict(modFitRF,vowel.test),vowel.test$y)
confusionMatrix(predict(modFitB,vowel.test),vowel.test$y)
confusionMatrix(predict(modFitRF,vowel.test),predict(modFitB,vowel.test))
set.seed(33833)
modFitRF = train(y~.,method="rf",data=vowel.train)
modFitB = train(y~.,method="gbm",data=vowel.train,verbose=FALSE)
confusionMatrix(predict(modFitRF,vowel.test),vowel.test$y)
confusionMatrix(predict(modFitB,vowel.test),vowel.test$y)
confusionMatrix(predict(modFitRF,vowel.test),predict(modFitB,vowel.test))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFitRF = train(diagnosis~.,method="rf",data=training)
modFitB = train(diagnosis~.,method="gbm",data=training)
modFitLD = train(diagnosis~.,method="lda",data=training)
modFitLD = train(diagnosis~.,method="lda",data=training)
modFitLD
set.seed(62433)
modFitRF = train(diagnosis~.,method="rf",data=training)
modFitB = train(diagnosis~.,method="gbm",data=training,verbose=FALSE)
modFitLD = train(diagnosis~.,method="lda",data=training)
pred1 = predict(modFitRF,testing)
pred2 = predict(modFitB,testing)
pred3 = predict(modFitLD,testing)
predDF = data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
names(predDF)
ComboModFit = train(diagnosis~.,method="rf",data=predDF)
predDF
predDF = data.frame(pred1,pred2,pred3,diagnosis=training$diagnosis)
predDF = data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
pred1
predict(modFitRF,testing)
table(pred1,testing$diagnosis)
confusionMatrix(pred1,testing$diagnosis)
names(confusionMatrix(pred1,testing$diagnosis))
confusionMatrix(pred1,testing$diagnosis)$overall
names(confusionMatrix(pred1,testing$diagnosis)$overall)
confusionMatrix(pred1,testing$diagnosis)$overall[[1]]
confusionMatrix(pred1,testing$diagnosis)$overall$Accuracy
confusionMatrix(pred1,testing$diagnosis)$overall[1]
confusionMatrix(pred1,testing$diagnosis)[[1]]
confusionMatrix(pred1,testing$diagnosis)$overall[1]
confusionMatrix(pred1,testing$diagnosis)$overall[1]
confusionMatrix(pred2,testing$diagnosis)$overall[1]
confusionMatrix(pred3,testing$diagnosis)$overall[1]
predc = predict(ComboModFit,predDF)
confusionMatrix(predc,testing$diagnosis)$overall[1]
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFitRF = train(diagnosis~.,method="rf",data=training)
modFitB = train(diagnosis~.,method="gbm",data=training,verbose=FALSE)
modFitLD = train(diagnosis~.,method="lda",data=training)
pred1 = predict(modFitRF,testing)
pred2 = predict(modFitB,testing)
pred3 = predict(modFitLD,testing)
predDF = data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
names(predDF)
ComboModFit = train(diagnosis~.,method="rf",data=predDF)
predc = predict(ComboModFit,predDF)
confusionMatrix(pred1,testing$diagnosis)$overall[1]
confusionMatrix(pred2,testing$diagnosis)$overall[1]
confusionMatrix(pred3,testing$diagnosis)$overall[1]
confusionMatrix(predc,testing$diagnosis)$overall[1]
predc
testing$diagnosis
confusionMatrix(pred1,testing$diagnosis)$overall[1]
confusionMatrix(pred2,testing$diagnosis)$overall[1]
confusionMatrix(pred3,testing$diagnosis)$overall[1]
confusionMatrix(predc,testing$diagnosis)$overall[1]
dat = read.csv("gaData.csv")
setwd("C:/Users/harric17/Desktop/R Stuff/coursera/machine_learning")
dat = read.csv("gaData.csv")
dat
training = dat[year(dat$date)  2011,]
tstrain = ts(training$visitsTumblr)
dat
names(dat)
head(dat)
year(dat$date)
training = dat[year(dat$date)==2011,]
class(dat$year)
type(dat$year)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
names(training)
fit = svm(CompressiveStrength~.)
names(training)
names(training)[-9]
fit = svm(CompressiveStrength~.,data=training)
table(predict(fit,testing$CompressiveStrength))
testing$CompressiveStrength
fit
predict(fit,testing$CompressiveStrength)
testing$CompressiveStrength
fit = svm(CompressiveStrength~.,data=training)
predict(fit,testing$CompressiveStrength)
fit
testing$CompressiveStrength
predict(fit,testing)
table(predict(fit,testing))
confusionMatrix(predict(fit,testing))
predict(fit,testing)
summary(predict(fit,testing))
predict(fit,testing)
(predict(fit,testing)-testing$CompressiveStrength)^2
sum((predict(fit,testing)-testing$CompressiveStrength)^2)
sum((predict(fit,testing)-testing$CompressiveStrength)^2)/length(testing$CompressiveStrength)
sqrt(sum((predict(fit,testing)-testing$CompressiveStrength)^2)/length(testing$CompressiveStrength))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
names(training)[-9]
fit = svm(CompressiveStrength~.,data=training)
sqrt(sum((predict(fit,testing)-testing$CompressiveStrength)^2)/length(testing$CompressiveStrength))
dat = read.csv("gaData.csv")
names(dat)
head(dat)
training = dat[year(dat$date)  2011,]
tstrain = ts(training$visitsTumblr)
dat$date
sapply(dat,class)
training = dat[year(as.numeric(dat$date))  2011,]
training = dat[year(as.numeric(dat$date))==2011,]
year(as.date(dat$date))
as.date(dat$date)
as.date(dat$date)
as.Date(dat$date)
year(as.Date(dat$date))
as.numeric(format(as.Date(dat$date), "%Y"))
as.numeric(format(as.Date(dat$date), "%Y"))==2011
training = dat[as.numeric(format(as.Date(dat$date), "%Y"))==2011,]
tstrain = ts(training$visitsTumblr)
training = dat[as.numeric(format(as.Date(dat$date), "%Y"))==2011,]
tstrain = ts(training$visitsTumblr)
dat = read.csv("gaData.csv")
names(dat)
head(dat)
training = dat[as.numeric(format(as.Date(dat$date), "%Y"))==2011,]
tstrain = ts(training$visitsTumblr)
head(dat)
bats(training)
install.packages(forecast)
install.packages("forecast")
library(forecast)
bats(training)
names(dat)
bats(training$visitsTumblr)
predict(bats(training$visitsTumblr),testing)
x=bats(training$visitsTumblr)
forecast(x)
forecast(x,testing)
forecast(x)
tstrain
forecast(x)
x=bats(training$visitsTumblr)
x
forecast(x)
accuracy(forecast(x),testing$visitsTumblr)
x=bats(training$visitsTumblr)
forecast(x)
length(training$visitsTumblr)
tstrain
training$visitsTumblr
head(dat)
tail(dat)
forecast(x)
tstrain
xxx = dat[as.numeric(format(as.Date(dat$date), "%Y"))==2012,]
head(xxx)
xxx[1:10]
xxx[1:10,]
forecast(x)
setwd("C:/Users/harric17/Desktop/R Stuff/coursera/data_products")
setwd("/Users/willharris/harric17.github.io/")
library(devtools)
library(slidify)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index2.Rmd')
library(slidify)
author("first_deck")
slidify('index.Rmd')
author()
author("/Users/willharris/harric17.github.io/")
slidify('index.Rmd')
author("/Users/willharris/harric17.github.io/")
slidify('index.Rmd')
